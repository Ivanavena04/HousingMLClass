# -*- coding: utf-8 -*-
"""script_entrenamiento

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RhQVgBUDQWnxqc_jUv19qfhbqVos8puN
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.metrics import mean_squared_error
import joblib
import os

# Cargar datos
df = pd.read_csv('/housing - copia - copia (3).csv')

# Crear una nueva columna 'income_cat' basada en 'median_income'
df['income_cat'] = pd.cut(df['median_income'],
                         bins=[0, 1.5, 3.0, 4.5, 6.0, np.inf],
                         labels=[1, 2, 3, 4, 5])
df['income_cat'].hist()

# Definir la partición estratificada
split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)

# Realizar la división estratificada
for train_index, test_index in split.split(df, df['income_cat']):
    strat_train_set = df.loc[train_index]
    strat_test_set = df.loc[test_index]

# Verificar la distribución de la categoría de ingreso
print(strat_test_set['income_cat'].value_counts() / len(strat_test_set))

# Remover atributo income_cat
for set_ in (strat_train_set, strat_test_set):
    set_.drop('income_cat', axis=1, inplace=True)

# Crear nuevos atributos
df["rooms_per_household"] = df["total_rooms"] / df["households"]
df["bedrooms_per_room"] = df["total_bedrooms"] / df["total_rooms"]
df["population_per_household"] = df["population"] / df["households"]

# Volver a un conjunto de entrenamiento limpio
df = strat_train_set.drop("median_house_value", axis=1)
df_labels = strat_train_set["median_house_value"].copy()

# Reemplazar los valores faltantes de cada atributo con la mediana de ese atributo
imputer = SimpleImputer(strategy="median")
# Crear una copia sin ocean proximity
df_num = df.drop("ocean_proximity", axis=1)
imputer.fit(df_num)

# Transformar el conjunto de entrenamiento reemplazando los valores faltantes con las medianas aprendidas
X = imputer.transform(df_num)
# El resultado es una matriz de numpy con las características transformadas
df_tr = pd.DataFrame(X, columns=df_num.columns, index=df_num.index)

# Seleccionar la columna categórica
df_cat = df[['ocean_proximity']]

# Crear un codificador one-hot
cat_encoder = OneHotEncoder(sparse=False)  # Usar sparse=False para obtener un array en lugar de una matriz dispersa

# Ajustar y transformar los datos
df_cat_1hot = cat_encoder.fit_transform(df_cat)

# Convertir a un DataFrame con nombres de columnas
df_cat_1hot_df = pd.DataFrame(df_cat_1hot, columns=cat_encoder.get_feature_names_out(['ocean_proximity']))
print(df_cat_1hot_df)

# Definición de CombinedAttributesAdder
class CombinedAttributesAdder(BaseEstimator, TransformerMixin):
    def __init__(self, add_bedrooms_per_room=True):
        self.add_bedrooms_per_room = add_bedrooms_per_room

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        rooms_ix = 0  # Assuming total_rooms is the first column
        households_ix = 1  # Assuming households is the second column
        population_ix = 2  # Assuming population is the third column
        bedrooms_ix = 3  # Assuming total_bedrooms is the fourth column

        rooms_per_household = X[:, rooms_ix] / np.maximum(X[:, households_ix], 1)
        population_per_household = X[:, population_ix] / np.maximum(X[:, households_ix], 1)

        if self.add_bedrooms_per_room:
            bedrooms_per_room = X[:, bedrooms_ix] / np.maximum(X[:, rooms_ix], 1)
            return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]
        else:
            return np.c_[X, rooms_per_household, population_per_household]

attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)
df_extra_attribs = attr_adder.transform(df.values)

# Crear el pipeline para datos numéricos
num_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy="median")),
    ('attribs_adder', CombinedAttributesAdder()),
    ('std_scaler', StandardScaler())
])

df_num_tr = num_pipeline.fit_transform(df_num)

# Crear el transformador para todas las columnas
num_attribs = list(df_num)
cat_attribs = ["ocean_proximity"]

full_pipeline = ColumnTransformer([
    ("num", num_pipeline, num_attribs),
    ("cat", OneHotEncoder(), cat_attribs),
])

# Preparar el conjunto de entrenamiento
df_prepared = full_pipeline.fit_transform(df)

# Definir el modelo de regresión lineal
lin_reg = LinearRegression()
lin_reg.fit(df_prepared, df_labels)

df_predictions = lin_reg.predict(df_prepared)
lin_mse = mean_squared_error(df_labels, df_predictions)
lin_rmse = np.sqrt(lin_mse)
print(f"RMSE: {lin_rmse:.2f}")

# Configurar GridSearchCV con LinearRegression
param_grid = [
    {'fit_intercept': [True, False]}  # Estos son los parámetros válidos para LinearRegression
]

grid_search = GridSearchCV(lin_reg, param_grid, cv=5,
                           scoring='neg_mean_squared_error',
                           return_train_score=True)

# Ajustar el modelo
grid_search.fit(df_prepared, df_labels)

# Mostrar los mejores parámetros encontrados
print("Best parameters found: ", grid_search.best_params_)
print("Best score found: ", grid_search.best_score_)

# Guardar el pipeline y el modelo
directory = 'model_files'
if not os.path.exists(directory):
    os.makedirs(directory)

# Guardar el pipeline
pipeline_path = os.path.join(directory, 'full_pipeline.pkl')
joblib.dump(full_pipeline, pipeline_path)
print(f"Pipeline guardado en: {pipeline_path}")

# Guardar el modelo de regresión
model_path = os.path.join(directory, 'lin_reg_model.pkl')
joblib.dump(grid_search.best_estimator_, model_path)
print(f"Modelo guardado en: {model_path}")

# Evaluar en el conjunto de prueba
final_model = grid_search.best_estimator_

# Asegúrate de que strat_test_set esté definido correctamente
x_test = strat_test_set.drop("median_house_value", axis=1)
y_test = strat_test_set["median_house_value"].copy()

x_test_prepared = full_pipeline.transform(x_test)

final_predictions = final_model.predict(x_test_prepared)

final_mse = mean_squared_error(y_test, final_predictions)
final_rmse = np.sqrt(final_mse)
print(f"Final RMSE: {final_rmse:.2f}")